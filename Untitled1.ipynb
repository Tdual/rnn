{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12838"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('qiita2')\n",
    "table.item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = table.scan()\n",
    "data = response['Items']\n",
    "\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    data.extend(response['Items'])\n",
    "articles = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment_count\n",
      "url\n",
      "stock_users\n",
      "private\n",
      "gist_url\n",
      "id\n",
      "updated_at\n",
      "user\n",
      "created_at_in_words\n",
      "updated_at_in_words\n",
      "body\n",
      "stocked\n",
      "stock_count\n",
      "raw_body\n",
      "tags\n",
      "title\n",
      "uuid\n",
      "created_at\n",
      "created_at_as_seconds\n",
      "tweet\n"
     ]
    }
   ],
   "source": [
    "for k in articles[0][\"json\"]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/0337fbcbff62fb8fa5d0b8be5c3b47d1115d91fc/medium.jpg?1418548649',\n",
       "  'name': 'Ruby',\n",
       "  'url_name': 'ruby',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/b8e309745fa0505b90b5256dfe2c0da1ef6fed7c/medium.jpg?1445349294',\n",
       "  'name': 'RubyOnRails',\n",
       "  'url_name': 'rubyonrails',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': '//cdn.qiita.com/assets/icons/medium/missing.png',\n",
       "  'name': 'Rails5',\n",
       "  'url_name': 'rails5',\n",
       "  'versions': []}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][\"json\"][\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37646\n",
      "37646\n"
     ]
    }
   ],
   "source": [
    "body_data = []\n",
    "tag_data = []\n",
    "for a in articles:\n",
    "    json = a[\"json\"]\n",
    "    body = json[\"raw_body\"]\n",
    "    tags = json[\"tags\"]\n",
    "    for tag in tags:        \n",
    "        body_data.append(body)\n",
    "        tag_data.append(tag[\"name\"])\n",
    "\n",
    "print(len(body_data))\n",
    "print(len(tag_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>icon_url</th>\n",
       "      <th>id</th>\n",
       "      <th>items_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17743</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>12523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30279</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>12357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16438</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Python</td>\n",
       "      <td>8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13196</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>iOS</td>\n",
       "      <td>8547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19287</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>PHP</td>\n",
       "      <td>8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   followers_count                                           icon_url  \\\n",
       "0            17743  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "1            30279  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "2            16438  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "3            13196  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "4            19287  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "\n",
       "           id  items_count  \n",
       "0        Ruby        12523  \n",
       "1  JavaScript        12357  \n",
       "2      Python         8890  \n",
       "3         iOS         8547  \n",
       "4         PHP         8233  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"tags.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list = df.ix[:,\"id\"].tolist()[:100]\n",
    "name_list.append(\"other\")\n",
    "one_hot = pd.get_dummies(name_list)\n",
    "one_hot.ix[:,\"Ruby\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37646"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data = []\n",
    "for t in tag_data:\n",
    "    if t not in name_list:\n",
    "        t = \"other\"\n",
    "    d = one_hot.ix[:,t]\n",
    "    one_hot_data.append(d)\n",
    "len(one_hot_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mac'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_name(one_hot):\n",
    "    one_idx = one_hot.idxmax()\n",
    "    return df.ix[one_idx,\"id\"]\n",
    "\n",
    "name = get_name(one_hot_data[20])\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#前提\\nmacOS Sierra 10.12.2\\n\\n#手順\\n\\n`Cmd+Shift+.` を押す。\\n\\n![finder.gif](https://qiita-image-store.s3.amazonaws.com/0/115844/6c0d8d55-f5e3-df03-d282-ca8710bd45f8.gif)\\n\\n参考にしたページ\\nhttp://qiita.com/kawaz/items/08b4f4770789fb96d70e\\n\\nいつの間にか素のFinderでもできるようになってたようです。\\n\\n---\\n以上\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_data[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_size = max([ len(c) for c in body_data ])\n",
    "#print(max_size)\n",
    "max_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "contents = []\n",
    "new_one_hot = []\n",
    "for b,oh in zip(body_data,one_hot_data):\n",
    "    word = tagger.parse(b).split(' ')\n",
    "    word = [ w.strip() for w in word ]\n",
    "    if len(word) < max_size:\n",
    "        contents.append(word)\n",
    "        new_one_hot.append(oh)\n",
    "    \n",
    "one_hot_data = new_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4990\n"
     ]
    }
   ],
   "source": [
    "padded_contents = [c +  [ '<PAD/>' ] * (max_size - len(c))for c in contents]\n",
    "print(max([ len(c) for c in padded_contents ]))\n",
    "contents = padded_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gem\n",
      "['##', '環境', 'OSX', '10', '.', '１', '１', '.', '6', 'Eclipse', 'neon', '##', '事象', 'Selenium', 'Webdriver', 'の', 'Eclipse', '環境', 'を', '構築', 'する', 'ため', '、', '[', 'こちら', '](', 'http://', 'qiita', '.com', '/', 'keigo', '1450', '/', 'items', '/', '77', 'db', '6', 'df', '3845', 'c', '991', 'dc', '346', ')', 'の', '記事', 'が', '非常', 'に', 'わかり', 'やすかっ', 'た', 'ので', '難なく', '環境', 'は', '出来', 'た', 'の', 'です', 'が', '、', 'いざ', 'FireFox', 'を', '動かそ', 'う', 'と', 'する', 'と', 'エラー', 'が', '発生', 'し', 'て', '困り', 'まし', 'た', '。', '##', 'エラー', '内容', '```', 'Exception', 'in', 'thread', '\"', 'main', '\"', 'java', '.', 'lang', '.', 'IllegalStateException', ':', 'The', 'path', 'to', 'the', 'driver', 'executable', 'must', 'be', 'set', 'by', 'the', 'webdriver', '.', 'gecko', '.', 'driver', 'system', 'property', ';', 'for', 'more', 'information', ',', 'see', 'https', '://', 'github', '.com', '/', 'mozilla', '/', 'geckodriver', '.', 'The', 'latest', 'version', 'can', 'be', 'downloaded', 'from', 'https', '://', 'github', '.com', '/', 'mozilla', '/', 'geckodriver', '/', 'releases', '・', '・', '・', '```', '##', '対処', '「', 'geckodriver', 'ない', 'ぞ', '」', 'と', 'の', 'こと', 'な', 'ので', '、', 'geckodriver', 'なんて', '聞い', 'た', 'こと', 'ない', 'けど', 'まあ', 'そう', '言わ', 'れ', 'てる', 'ん', 'だ', 'から', '素直', 'に', '探そ', 'う', 'という', 'こと', 'で', '、', '下記', 'に', '行き着き', '素直', 'に', 'Mac', '用', 'の', 'を', 'ダウンロード', 'し', 'まし', 'た', '。', '[', 'https', '://', 'github', '.com', '/', 'mozilla', '/', 'geckodriver', '/', 'releases', '](', 'https', '://', 'github', '.com', '/', 'mozilla', '/', 'geckodriver', '/', 'releases', ')', 'そして', 'Java', 'の', 'システムプロパティ', 'で', 'PATH', 'を', '通し', 'ます', '。', '```', 'System', '.s', 'etProperty', '(\"', 'webdriver', '.', 'gecko', '.', 'driver', '\",', '\"/（', 'geckodriver', 'の', '格納', '場所', '）/', 'geckodriver', '\");', '```', 'そして', '再', '実行', 'する', 'と', '、', '、', '、', '##', '結果', '無事', '動き', 'まし', 'た', '。', '素直', 'って', '大切', 'です', 'ね', '。', '', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>', '<PAD/>']\n"
     ]
    }
   ],
   "source": [
    "test_idx = 1\n",
    "print(get_name(one_hot_data[test_idx]))\n",
    "print(contents[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36415\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(one_hot_data)*0.999)\n",
    "test_size = len(one_hot_data) - train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "ctr = Counter(itertools.chain(*contents))\n",
    "dictionaries = [c[0] for c in ctr.most_common()]\n",
    "dictionaries_inv = { c: i for i, c in enumerate(dictionaries) }\n",
    "\n",
    "data = [[dictionaries_inv[word] for word in content ] for content in contents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "     epoch: 10\n",
      "mini batch: 64\n",
      "train data: 36415\n",
      " test data: 37\n",
      "We will loop 569 count per an epoch.\n",
      "Start 1th epoch.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1ca78fa134dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m             _, v1, v2, v3, v4 = sess.run(\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m[\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccr_sum\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmini_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmini_batch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             )\n\u001b[1;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%4dth mini batch complete. LOSS: %f, ACCR: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "train_x = data[:train_size]\n",
    "train_y = one_hot_data[:train_size]\n",
    "test_x = data[train_size:]\n",
    "test_y = one_hot_data[train_size:]\n",
    "NUM_CLASSES = 101\n",
    "\n",
    "NUM_TESTS         = 2000\n",
    "NUM_EPOCHS        = 10\n",
    "NUM_MINI_BATCH    = 64\n",
    "EMBEDDING_SIZE    = 1280\n",
    "NUM_FILTERS       = 1280\n",
    "FILTER_SIZES      = [3,4,5]\n",
    "L2_LAMBDA         = 0.0001\n",
    "EVALUATE_EVERY    = 100\n",
    "CHECKPOINTS_EVERY = 1000\n",
    "SUMMARY_LOG_DIR = \"tmp/tensorflow_log\"\n",
    "CHECKPOINTS_DIR = './'\n",
    "\n",
    "keep = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_x = tf.placeholder(tf.int32, [None, max_size])\n",
    "input_y = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "\n",
    "with tf.name_scope('embedding'):\n",
    "    w  = tf.Variable(tf.random_uniform([len(dictionaries), EMBEDDING_SIZE], -1.0, 1.0), name='weight')\n",
    "    e  = tf.nn.embedding_lookup(w, input_x)\n",
    "    ex = tf.expand_dims(e, -1)\n",
    "\n",
    "# Define 3rd and 4th layer (Temporal 1-D convolutional and max-pooling layer).\n",
    "p_array = []\n",
    "for filter_size in FILTER_SIZES:\n",
    "    with tf.name_scope('conv-%d' % filter_size):\n",
    "        w  = tf.Variable(tf.truncated_normal([ filter_size, EMBEDDING_SIZE, 1, NUM_FILTERS ], stddev=0.02), name='weight')\n",
    "        b  = tf.Variable(tf.constant(0.1, shape=[ NUM_FILTERS ]), name='bias')\n",
    "        c0 = tf.nn.conv2d(ex, w, [ 1, 1, 1, 1 ], 'VALID')\n",
    "        c1 = tf.nn.relu(tf.nn.bias_add(c0, b))\n",
    "        c2 = tf.nn.max_pool(c1, [ 1,  max_size - filter_size + 1, 1, 1 ], [ 1, 1, 1, 1 ], 'VALID')\n",
    "        p_array.append(c2)\n",
    "\n",
    "p = tf.concat(3, p_array)\n",
    "\n",
    "\n",
    "with tf.name_scope('fc'):\n",
    "    total_filters = NUM_FILTERS * len(FILTER_SIZES)\n",
    "    w = tf.Variable(tf.truncated_normal([ total_filters, NUM_CLASSES ], stddev=0.02), name='weight')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[ NUM_CLASSES ]), name='bias')\n",
    "    h0 = tf.nn.dropout(tf.reshape(p, [ -1, total_filters ]), keep)\n",
    "    predict_y = tf.nn.softmax(tf.matmul(h0, w) + b)\n",
    "\n",
    "\n",
    "xentropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predict_y, input_y))\n",
    "loss = xentropy + L2_LAMBDA * tf.nn.l2_loss(w)\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "train = tf.train.AdamOptimizer(0.0001).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "predict  = tf.equal(tf.argmax(predict_y, 1), tf.argmax(input_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n",
    "\n",
    "loss_sum   = tf.scalar_summary('train loss', loss)\n",
    "accr_sum   = tf.scalar_summary('train accuracy', accuracy)\n",
    "t_loss_sum = tf.scalar_summary('general loss', loss)\n",
    "t_accr_sum = tf.scalar_summary('general accuracy', accuracy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter(SUMMARY_LOG_DIR, sess.graph)\n",
    "\n",
    "    train_x_length = len(train_x)\n",
    "    batch_count = int(train_x_length / NUM_MINI_BATCH) + 1\n",
    "\n",
    "    print('Start training.')\n",
    "    print('     epoch: %d' % NUM_EPOCHS)\n",
    "    print('mini batch: %d' % NUM_MINI_BATCH)\n",
    "    print('train data: %d' % train_x_length)\n",
    "    print(' test data: %d' % len(test_x))\n",
    "    print('We will loop %d count per an epoch.' % batch_count)\n",
    "\n",
    "   \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        random_indice = np.random.permutation(train_x_length)\n",
    "        print('Start %dth epoch.' % (epoch + 1))\n",
    "        for i in range(batch_count):\n",
    "            mini_batch_x = []\n",
    "            mini_batch_y = []\n",
    "            for j in range(min(train_x_length - i * NUM_MINI_BATCH, NUM_MINI_BATCH)):\n",
    "                mini_batch_x.append(train_x[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "                mini_batch_y.append(train_y[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "\n",
    "            \n",
    "            _, v1, v2, v3, v4 = sess.run(\n",
    "                [ train, loss, accuracy, loss_sum, accr_sum ],\n",
    "                feed_dict={ input_x: mini_batch_x, input_y: mini_batch_y, keep: 0.5 }\n",
    "            )\n",
    "            print('%4dth mini batch complete. LOSS: %f, ACCR: %f' % (i + 1, v1, v2))\n",
    "\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            writer.add_summary(v3, current_step)\n",
    "            writer.add_summary(v4, current_step)\n",
    "\n",
    "\n",
    "            if current_step % CHECKPOINTS_EVERY == 0:\n",
    "                saver.save(sess, CHECKPOINTS_DIR + '/model', global_step=current_step)\n",
    "                print('Checkout was completed.')\n",
    "\n",
    " \n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                #random_test_indice = np.random.permutation(100)\n",
    "                #random_test_x = test_x[int(random_test_indice)]\n",
    "                #random_test_y = test_y[int(random_test_indice)]\n",
    "\n",
    "                v1, v2, v3, v4 = sess.run(\n",
    "                    [ loss, accuracy, t_loss_sum, t_accr_sum ],\n",
    "                    feed_dict={ input_x: test_x, input_y: test_y, keep: 1.0 }\n",
    "                )\n",
    "                print('Testing... LOSS: %f, ACCR: %f' % (v1, v2))\n",
    "                writer.add_summary(v3, current_step)\n",
    "                writer.add_summary(v4, current_step)\n",
    "\n",
    "\n",
    "    saver.save(sess, CHECKPOINTS_DIR + '/model-last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training_predictors_tf = body_data[:train_size]\n",
    "#test_predictors_tf = body_data[train_size:]\n",
    "#training_classes_tf = one_hot_data[:train_size]\n",
    "#test_classes_tf = one_hot_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (3,) and (4,) are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m           \u001b[0mnew_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    107\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\"\n\u001b[0;32m--> 108\u001b[0;31m                        % (self, other))\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3 and 4 are not compatible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0msupervisor_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"supervisor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupervisor_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_ph)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36munpack_sequence\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   2487\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m   \"\"\"\n\u001b[0;32m-> 2489\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    701\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    702\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    704\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2310\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1703\u001b[0m                          % op.type)\n\u001b[0;32m-> 1704\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_TransposeShape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2117\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m   transpose_shape = op.inputs[1].get_shape().merge_with(tensor_shape.vector(\n\u001b[0;32m-> 2119\u001b[0;31m       input_shape.ndims))\n\u001b[0m\u001b[1;32m   2120\u001b[0m   \u001b[0mtranspose_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         raise ValueError(\"Shapes %s and %s are not compatible\" %\n\u001b[0;32m--> 570\u001b[0;31m                          (self, other))\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (3,) and (4,) are not compatible"
     ]
    }
   ],
   "source": [
    "!rm -rf tmp/tensorflow_log/*\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "num_of_input_nodes = 1\n",
    "num_of_hidden_nodes = 101\n",
    "num_of_output_nodes = 1\n",
    "num_of_training_epochs = 50000\n",
    "batch_size = 100\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.001\n",
    "forget_bias = 0.9\n",
    "num_of_sample = 1000\n",
    "num_layers = 1\n",
    "\n",
    "batch_size = 100\n",
    "sequences_length = 30 \n",
    "test_num = int(num_of_sample*0.3)\n",
    "class_num = 101\n",
    "\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128 \n",
    "\n",
    "\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def create_batch(batch_size, X, t):\n",
    "    #X = X.as_matrix()\n",
    "    #t = t.as_matrix()\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def unpack_sequence(tensor):\n",
    "    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def pack_sequence(sequence):\n",
    "    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "def inference(input_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        in_size = num_of_hidden_nodes\n",
    "        out_size = class_num\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocabulary_size, embedding_size])\n",
    "        \n",
    "       \n",
    "        # network = tf.nn.rnn_cell.LSTMCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.GRUCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(network, output_keep_prob=0.5)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * num_layers)\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_ph)\n",
    "        inputs =  unpack_sequence(inputs)\n",
    "       \n",
    "        rnn_output, states_op = tf.nn.rnn(network,inputs,dtype=tf.int32)\n",
    "        #rnn_output, states_op = tf.nn.dynamic_rnn(network,inputs,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        #rnn_output = pack_sequence(rnn_output)\n",
    "        #state_op = pack_sequence(states_op)\n",
    "        \n",
    "        output_op = tf.transpose(rnn_output, [1, 0, 2])\n",
    "        #output_op = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        #output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight) + bias)\n",
    "\n",
    " \n",
    "        tf.histogram_summary(\"weights\", weight)\n",
    "        tf.histogram_summary(\"biases\", bias)\n",
    "        tf.histogram_summary(\"output\",  output_op)\n",
    "        results = [weight, bias]\n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss_op = - tf.reduce_sum(supervisor_ph * tf.log(output_op))\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "def accuracy(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(output_op,1), tf.argmax(supervisor_ph,1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "        return accuracy_op\n",
    "\n",
    "def calc_accuracy(accuracy_opp, X, t):\n",
    "    inputs, targets = create_batch(len(X), X, t)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: targets\n",
    "    }\n",
    "    accurecy = sess.run(accuracy_op, feed_dict=pred_dict)\n",
    "    print(accurecy)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.int32, [None, sequences_length, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, class_num], name=\"supervisor\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "    accuracy_op = accuracy(output_op, supervisor_ph)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            #inputs, supervisors = create_batch(batch_size, training_predictors_tf , training_classes_tf)\n",
    "            inputs, supervisors = training_predictors_tf , training_classes_tf\n",
    "            train_dict = {\n",
    "                input_ph:   inputs,\n",
    "                supervisor_ph: supervisors\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 1000 == 0:\n",
    "                #summary_str, loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                loss = sess.run(loss_op, feed_dict=train_dict)\n",
    "                print(\"train#{}, loss: {}\".format(epoch, loss))\n",
    "                #summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 5000 == 0:\n",
    "                    calc_accuracy(output_op, test_predictors_tf, test_classes_tf)\n",
    "        calc_accuracy(output_op, X_test, t_test)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
