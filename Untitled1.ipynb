{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12838"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('qiita2')\n",
    "table.item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = table.scan()\n",
    "data = response['Items']\n",
    "\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    data.extend(response['Items'])\n",
    "articles = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "gist_url\n",
      "created_at_in_words\n",
      "uuid\n",
      "stocked\n",
      "comment_count\n",
      "stock_users\n",
      "created_at\n",
      "url\n",
      "raw_body\n",
      "body\n",
      "stock_count\n",
      "tags\n",
      "user\n",
      "updated_at_in_words\n",
      "created_at_as_seconds\n",
      "tweet\n",
      "updated_at\n",
      "private\n",
      "id\n"
     ]
    }
   ],
   "source": [
    "for k in articles[0][\"json\"]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/f9384932787d996b6e1ed26d7d0b2bc2a9e36441/medium.jpg?1443704516',\n",
       "  'name': '初心者',\n",
       "  'url_name': '%e5%88%9d%e5%bf%83%e8%80%85',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': '//cdn.qiita.com/assets/icons/medium/missing.png',\n",
       "  'name': 'selenium-webdriver',\n",
       "  'url_name': 'selenium-webdriver',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': '//cdn.qiita.com/assets/icons/medium/missing.png',\n",
       "  'name': 'GeckoDriver',\n",
       "  'url_name': 'geckodriver',\n",
       "  'versions': []}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0][\"json\"][\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37646"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_data = []\n",
    "tag_data = []\n",
    "for a in articles:\n",
    "    json = a[\"json\"]\n",
    "    body = json[\"raw_body\"]\n",
    "    tags = json[\"tags\"]\n",
    "    for tag in tags:        \n",
    "        body_data.append(body)\n",
    "        tag_data.append(tag[\"name\"])\n",
    "\n",
    "len(body_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>icon_url</th>\n",
       "      <th>id</th>\n",
       "      <th>items_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17743</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>12523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30279</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>12357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16438</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Python</td>\n",
       "      <td>8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13196</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>iOS</td>\n",
       "      <td>8547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19287</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>PHP</td>\n",
       "      <td>8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   followers_count                                           icon_url  \\\n",
       "0            17743  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "1            30279  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "2            16438  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "3            13196  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "4            19287  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "\n",
       "           id  items_count  \n",
       "0        Ruby        12523  \n",
       "1  JavaScript        12357  \n",
       "2      Python         8890  \n",
       "3         iOS         8547  \n",
       "4         PHP         8233  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"tags.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list = df.ix[:,\"id\"].tolist()[:100]\n",
    "name_list.append(\"other\")\n",
    "one_hot = pd.get_dummies(name_list)\n",
    "one_hot.ix[:,\"Ruby\"]\n",
    "one_hot.ix[:,\"Ruby\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data = []\n",
    "for t in tag_data:\n",
    "    if t not in name_list:\n",
    "        t = \"other\"\n",
    "    d = one_hot.ix[:,t]\n",
    "    one_hot_data.append(d)\n",
    "len(one_hot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#training_predictors_tf = body_data[:train_size]\n",
    "#test_predictors_tf = body_data[train_size:]\n",
    "#training_classes_tf = one_hot_data[:train_size]\n",
    "#test_classes_tf = one_hot_data[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "contents = []\n",
    "new_one_hot = []\n",
    "for b,oh in zip(body_data,one_hot_data):\n",
    "    word = tagger.parse(b).split(' ')\n",
    "    word = [ w.strip() for w in word ]\n",
    "    if len(word) < 1000:\n",
    "        contents.append(word)\n",
    "        new_one_hot.append(oh)\n",
    "    \n",
    "one_hot_data = new_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "#max_size = max([ len(c) for c in body_data ])\n",
    "#print(max_size)\n",
    "max_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "padded_contents = [c +  [ '<PAD/>' ] * (max_size - len(c))for c in contents]\n",
    "print(max([ len(c) for c in padded_contents ]))\n",
    "contents = padded_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19481\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(one_hot_data)*0.997)\n",
    "test_size = len(one_hot_data) - train_size\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "ctr = Counter(itertools.chain(*contents))\n",
    "dictionaries = [c[0] for c in ctr.most_common()]\n",
    "dictionaries_inv = { c: i for i, c in enumerate(dictionaries) }\n",
    "\n",
    "data = [ [ dictionaries_inv[word] for word in content ] for content in contents ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "     epoch: 10\n",
      "mini batch: 64\n",
      "train data: 19481\n",
      " test data: 59\n",
      "We will loop 305 count per an epoch.\n",
      "Start 1th epoch.\n",
      "   1th mini batch complete. LOSS: 4.614492, ACCR: 0.000000\n",
      "   2th mini batch complete. LOSS: 4.614471, ACCR: 0.031250\n",
      "   3th mini batch complete. LOSS: 4.614166, ACCR: 0.031250\n",
      "   4th mini batch complete. LOSS: 4.613424, ACCR: 0.031250\n",
      "   5th mini batch complete. LOSS: 4.613190, ACCR: 0.031250\n",
      "   6th mini batch complete. LOSS: 4.613107, ACCR: 0.046875\n",
      "   7th mini batch complete. LOSS: 4.611171, ACCR: 0.171875\n",
      "   8th mini batch complete. LOSS: 4.612095, ACCR: 0.109375\n",
      "   9th mini batch complete. LOSS: 4.609909, ACCR: 0.203125\n",
      "  10th mini batch complete. LOSS: 4.610597, ACCR: 0.156250\n",
      "  11th mini batch complete. LOSS: 4.606496, ACCR: 0.281250\n",
      "  12th mini batch complete. LOSS: 4.606765, ACCR: 0.296875\n",
      "  13th mini batch complete. LOSS: 4.608328, ACCR: 0.265625\n",
      "  14th mini batch complete. LOSS: 4.608858, ACCR: 0.281250\n",
      "  15th mini batch complete. LOSS: 4.606309, ACCR: 0.359375\n",
      "  16th mini batch complete. LOSS: 4.606137, ACCR: 0.328125\n",
      "  17th mini batch complete. LOSS: 4.601296, ACCR: 0.515625\n",
      "  18th mini batch complete. LOSS: 4.604229, ACCR: 0.375000\n",
      "  19th mini batch complete. LOSS: 4.604043, ACCR: 0.312500\n",
      "  20th mini batch complete. LOSS: 4.596728, ACCR: 0.500000\n",
      "  21th mini batch complete. LOSS: 4.597224, ACCR: 0.468750\n",
      "  22th mini batch complete. LOSS: 4.595772, ACCR: 0.484375\n",
      "  23th mini batch complete. LOSS: 4.596576, ACCR: 0.468750\n",
      "  24th mini batch complete. LOSS: 4.592505, ACCR: 0.406250\n",
      "  25th mini batch complete. LOSS: 4.579318, ACCR: 0.640625\n",
      "  26th mini batch complete. LOSS: 4.589343, ACCR: 0.437500\n",
      "  27th mini batch complete. LOSS: 4.572616, ACCR: 0.546875\n",
      "  28th mini batch complete. LOSS: 4.573212, ACCR: 0.546875\n",
      "  29th mini batch complete. LOSS: 4.561576, ACCR: 0.609375\n",
      "  30th mini batch complete. LOSS: 4.580655, ACCR: 0.375000\n",
      "  31th mini batch complete. LOSS: 4.566021, ACCR: 0.500000\n",
      "  32th mini batch complete. LOSS: 4.554530, ACCR: 0.546875\n",
      "  33th mini batch complete. LOSS: 4.547695, ACCR: 0.562500\n",
      "  34th mini batch complete. LOSS: 4.522614, ACCR: 0.593750\n",
      "  35th mini batch complete. LOSS: 4.540419, ACCR: 0.453125\n",
      "  36th mini batch complete. LOSS: 4.518456, ACCR: 0.531250\n",
      "  37th mini batch complete. LOSS: 4.533506, ACCR: 0.468750\n",
      "  38th mini batch complete. LOSS: 4.518020, ACCR: 0.437500\n",
      "  39th mini batch complete. LOSS: 4.504367, ACCR: 0.437500\n",
      "  40th mini batch complete. LOSS: 4.475970, ACCR: 0.484375\n",
      "  41th mini batch complete. LOSS: 4.461209, ACCR: 0.484375\n",
      "  42th mini batch complete. LOSS: 4.397766, ACCR: 0.640625\n",
      "  43th mini batch complete. LOSS: 4.423992, ACCR: 0.531250\n",
      "  44th mini batch complete. LOSS: 4.445197, ACCR: 0.453125\n",
      "  45th mini batch complete. LOSS: 4.421060, ACCR: 0.484375\n",
      "  46th mini batch complete. LOSS: 4.359589, ACCR: 0.500000\n",
      "  47th mini batch complete. LOSS: 4.306509, ACCR: 0.593750\n",
      "  48th mini batch complete. LOSS: 4.290920, ACCR: 0.578125\n",
      "  49th mini batch complete. LOSS: 4.337501, ACCR: 0.468750\n",
      "  50th mini batch complete. LOSS: 4.340336, ACCR: 0.453125\n",
      "  51th mini batch complete. LOSS: 4.269452, ACCR: 0.562500\n",
      "  52th mini batch complete. LOSS: 4.368153, ACCR: 0.390625\n",
      "  53th mini batch complete. LOSS: 4.297039, ACCR: 0.421875\n",
      "  54th mini batch complete. LOSS: 4.197763, ACCR: 0.578125\n",
      "  55th mini batch complete. LOSS: 4.202616, ACCR: 0.515625\n",
      "  56th mini batch complete. LOSS: 4.211205, ACCR: 0.500000\n",
      "  57th mini batch complete. LOSS: 4.133792, ACCR: 0.562500\n",
      "  58th mini batch complete. LOSS: 4.205501, ACCR: 0.484375\n",
      "  59th mini batch complete. LOSS: 4.090462, ACCR: 0.625000\n",
      "  60th mini batch complete. LOSS: 4.239707, ACCR: 0.437500\n",
      "  61th mini batch complete. LOSS: 4.194918, ACCR: 0.500000\n",
      "  62th mini batch complete. LOSS: 4.040051, ACCR: 0.640625\n",
      "  63th mini batch complete. LOSS: 4.236415, ACCR: 0.437500\n",
      "  64th mini batch complete. LOSS: 4.104917, ACCR: 0.562500\n",
      "  65th mini batch complete. LOSS: 4.168959, ACCR: 0.484375\n",
      "  66th mini batch complete. LOSS: 4.100617, ACCR: 0.562500\n",
      "  67th mini batch complete. LOSS: 4.163136, ACCR: 0.484375\n",
      "  68th mini batch complete. LOSS: 4.099400, ACCR: 0.562500\n",
      "  69th mini batch complete. LOSS: 4.184229, ACCR: 0.468750\n",
      "  70th mini batch complete. LOSS: 4.187341, ACCR: 0.468750\n",
      "  71th mini batch complete. LOSS: 4.183352, ACCR: 0.468750\n",
      "  72th mini batch complete. LOSS: 4.110835, ACCR: 0.546875\n",
      "  73th mini batch complete. LOSS: 4.112119, ACCR: 0.531250\n",
      "  74th mini batch complete. LOSS: 4.118724, ACCR: 0.531250\n",
      "  75th mini batch complete. LOSS: 4.066267, ACCR: 0.578125\n",
      "  76th mini batch complete. LOSS: 4.187018, ACCR: 0.453125\n",
      "  77th mini batch complete. LOSS: 4.080199, ACCR: 0.562500\n",
      "  78th mini batch complete. LOSS: 4.114742, ACCR: 0.531250\n",
      "  79th mini batch complete. LOSS: 4.124350, ACCR: 0.515625\n",
      "  80th mini batch complete. LOSS: 4.093965, ACCR: 0.546875\n",
      "  81th mini batch complete. LOSS: 4.099125, ACCR: 0.546875\n",
      "  82th mini batch complete. LOSS: 4.145322, ACCR: 0.500000\n",
      "  83th mini batch complete. LOSS: 4.171682, ACCR: 0.468750\n",
      "  84th mini batch complete. LOSS: 4.158176, ACCR: 0.484375\n",
      "  85th mini batch complete. LOSS: 4.168956, ACCR: 0.468750\n",
      "  86th mini batch complete. LOSS: 4.264014, ACCR: 0.375000\n",
      "  87th mini batch complete. LOSS: 4.199468, ACCR: 0.437500\n",
      "  88th mini batch complete. LOSS: 4.122020, ACCR: 0.515625\n",
      "  89th mini batch complete. LOSS: 4.091564, ACCR: 0.546875\n",
      "  90th mini batch complete. LOSS: 4.105961, ACCR: 0.531250\n",
      "  91th mini batch complete. LOSS: 4.121987, ACCR: 0.515625\n",
      "  92th mini batch complete. LOSS: 4.075859, ACCR: 0.562500\n",
      "  93th mini batch complete. LOSS: 4.138870, ACCR: 0.500000\n",
      "  94th mini batch complete. LOSS: 4.120560, ACCR: 0.515625\n",
      "  95th mini batch complete. LOSS: 4.032674, ACCR: 0.609375\n",
      "  96th mini batch complete. LOSS: 4.075930, ACCR: 0.562500\n",
      "  97th mini batch complete. LOSS: 4.120382, ACCR: 0.515625\n",
      "  98th mini batch complete. LOSS: 4.108777, ACCR: 0.531250\n",
      "  99th mini batch complete. LOSS: 4.167524, ACCR: 0.468750\n",
      " 100th mini batch complete. LOSS: 4.183873, ACCR: 0.453125\n",
      "Testing... LOSS: 4.244483, ACCR: 0.389830\n",
      " 101th mini batch complete. LOSS: 4.075521, ACCR: 0.562500\n",
      " 102th mini batch complete. LOSS: 4.246072, ACCR: 0.390625\n",
      " 103th mini batch complete. LOSS: 4.026129, ACCR: 0.609375\n",
      " 104th mini batch complete. LOSS: 4.166139, ACCR: 0.468750\n",
      " 105th mini batch complete. LOSS: 4.118886, ACCR: 0.515625\n",
      " 106th mini batch complete. LOSS: 4.028083, ACCR: 0.609375\n",
      " 107th mini batch complete. LOSS: 4.045004, ACCR: 0.593750\n",
      " 108th mini batch complete. LOSS: 4.243461, ACCR: 0.390625\n",
      " 109th mini batch complete. LOSS: 4.167610, ACCR: 0.468750\n",
      " 110th mini batch complete. LOSS: 4.135302, ACCR: 0.500000\n",
      " 111th mini batch complete. LOSS: 4.151116, ACCR: 0.484375\n",
      " 112th mini batch complete. LOSS: 4.079775, ACCR: 0.562500\n",
      " 113th mini batch complete. LOSS: 4.104939, ACCR: 0.531250\n",
      " 114th mini batch complete. LOSS: 4.074933, ACCR: 0.562500\n",
      " 115th mini batch complete. LOSS: 4.111135, ACCR: 0.531250\n",
      " 116th mini batch complete. LOSS: 4.150310, ACCR: 0.484375\n",
      " 117th mini batch complete. LOSS: 4.072547, ACCR: 0.562500\n",
      " 118th mini batch complete. LOSS: 4.042804, ACCR: 0.593750\n",
      " 119th mini batch complete. LOSS: 3.967516, ACCR: 0.671875\n",
      " 120th mini batch complete. LOSS: 4.119735, ACCR: 0.515625\n",
      " 121th mini batch complete. LOSS: 4.166485, ACCR: 0.468750\n",
      " 122th mini batch complete. LOSS: 4.353918, ACCR: 0.281250\n",
      " 123th mini batch complete. LOSS: 4.058331, ACCR: 0.578125\n",
      " 124th mini batch complete. LOSS: 4.102805, ACCR: 0.531250\n",
      " 125th mini batch complete. LOSS: 4.072866, ACCR: 0.562500\n",
      " 126th mini batch complete. LOSS: 4.151227, ACCR: 0.484375\n",
      " 127th mini batch complete. LOSS: 4.040926, ACCR: 0.593750\n",
      " 128th mini batch complete. LOSS: 4.103697, ACCR: 0.531250\n",
      " 129th mini batch complete. LOSS: 4.149645, ACCR: 0.484375\n",
      " 130th mini batch complete. LOSS: 4.091274, ACCR: 0.546875\n",
      " 131th mini batch complete. LOSS: 4.134264, ACCR: 0.500000\n",
      " 132th mini batch complete. LOSS: 4.097249, ACCR: 0.546875\n",
      " 133th mini batch complete. LOSS: 4.071689, ACCR: 0.562500\n",
      " 134th mini batch complete. LOSS: 4.133648, ACCR: 0.500000\n",
      " 135th mini batch complete. LOSS: 4.027668, ACCR: 0.609375\n",
      " 136th mini batch complete. LOSS: 4.168003, ACCR: 0.468750\n",
      " 137th mini batch complete. LOSS: 4.229255, ACCR: 0.406250\n",
      " 138th mini batch complete. LOSS: 4.103306, ACCR: 0.531250\n",
      " 139th mini batch complete. LOSS: 4.196112, ACCR: 0.437500\n",
      " 140th mini batch complete. LOSS: 4.243046, ACCR: 0.390625\n",
      " 141th mini batch complete. LOSS: 4.071923, ACCR: 0.562500\n",
      " 142th mini batch complete. LOSS: 4.212508, ACCR: 0.421875\n",
      " 143th mini batch complete. LOSS: 4.134701, ACCR: 0.500000\n",
      " 144th mini batch complete. LOSS: 4.056941, ACCR: 0.578125\n",
      " 145th mini batch complete. LOSS: 4.150721, ACCR: 0.484375\n",
      " 146th mini batch complete. LOSS: 4.165874, ACCR: 0.468750\n",
      " 147th mini batch complete. LOSS: 4.072028, ACCR: 0.562500\n",
      " 148th mini batch complete. LOSS: 4.150405, ACCR: 0.484375\n",
      " 149th mini batch complete. LOSS: 4.181006, ACCR: 0.453125\n",
      " 150th mini batch complete. LOSS: 4.195792, ACCR: 0.437500\n",
      " 151th mini batch complete. LOSS: 4.071658, ACCR: 0.562500\n",
      " 152th mini batch complete. LOSS: 4.102561, ACCR: 0.531250\n",
      " 153th mini batch complete. LOSS: 4.103689, ACCR: 0.531250\n",
      " 154th mini batch complete. LOSS: 4.072526, ACCR: 0.562500\n",
      " 155th mini batch complete. LOSS: 4.120031, ACCR: 0.515625\n",
      " 156th mini batch complete. LOSS: 4.150880, ACCR: 0.484375\n",
      " 157th mini batch complete. LOSS: 4.165919, ACCR: 0.468750\n",
      " 158th mini batch complete. LOSS: 4.102462, ACCR: 0.531250\n",
      " 159th mini batch complete. LOSS: 4.212176, ACCR: 0.421875\n",
      " 160th mini batch complete. LOSS: 4.103979, ACCR: 0.531250\n",
      " 161th mini batch complete. LOSS: 4.102830, ACCR: 0.531250\n",
      " 162th mini batch complete. LOSS: 4.071440, ACCR: 0.562500\n",
      " 163th mini batch complete. LOSS: 4.149261, ACCR: 0.484375\n",
      " 164th mini batch complete. LOSS: 4.071367, ACCR: 0.562500\n",
      " 165th mini batch complete. LOSS: 4.196202, ACCR: 0.437500\n",
      " 166th mini batch complete. LOSS: 4.087951, ACCR: 0.546875\n",
      " 167th mini batch complete. LOSS: 4.259473, ACCR: 0.375000\n",
      " 168th mini batch complete. LOSS: 4.118797, ACCR: 0.515625\n",
      " 169th mini batch complete. LOSS: 4.071815, ACCR: 0.562500\n",
      " 170th mini batch complete. LOSS: 4.056437, ACCR: 0.578125\n",
      " 171th mini batch complete. LOSS: 4.087627, ACCR: 0.546875\n",
      " 172th mini batch complete. LOSS: 4.180144, ACCR: 0.453125\n",
      " 173th mini batch complete. LOSS: 4.196040, ACCR: 0.437500\n",
      " 174th mini batch complete. LOSS: 3.978506, ACCR: 0.656250\n",
      " 175th mini batch complete. LOSS: 4.180414, ACCR: 0.453125\n",
      " 176th mini batch complete. LOSS: 4.087254, ACCR: 0.546875\n",
      " 177th mini batch complete. LOSS: 4.133875, ACCR: 0.500000\n",
      " 178th mini batch complete. LOSS: 4.118691, ACCR: 0.515625\n",
      " 179th mini batch complete. LOSS: 4.044943, ACCR: 0.593750\n",
      " 180th mini batch complete. LOSS: 4.134627, ACCR: 0.500000\n",
      " 181th mini batch complete. LOSS: 4.164951, ACCR: 0.468750\n",
      " 182th mini batch complete. LOSS: 4.087652, ACCR: 0.546875\n",
      " 183th mini batch complete. LOSS: 4.087515, ACCR: 0.546875\n",
      " 184th mini batch complete. LOSS: 4.133664, ACCR: 0.500000\n",
      " 185th mini batch complete. LOSS: 4.039992, ACCR: 0.593750\n",
      " 186th mini batch complete. LOSS: 4.151032, ACCR: 0.484375\n",
      " 187th mini batch complete. LOSS: 4.149036, ACCR: 0.484375\n",
      " 188th mini batch complete. LOSS: 4.133906, ACCR: 0.500000\n",
      " 189th mini batch complete. LOSS: 4.024253, ACCR: 0.609375\n",
      " 190th mini batch complete. LOSS: 4.088020, ACCR: 0.546875\n",
      " 191th mini batch complete. LOSS: 4.089202, ACCR: 0.546875\n",
      " 192th mini batch complete. LOSS: 4.148920, ACCR: 0.484375\n",
      " 193th mini batch complete. LOSS: 4.103688, ACCR: 0.531250\n",
      " 194th mini batch complete. LOSS: 4.149322, ACCR: 0.484375\n",
      " 195th mini batch complete. LOSS: 4.119705, ACCR: 0.515625\n",
      " 196th mini batch complete. LOSS: 4.039920, ACCR: 0.593750\n",
      " 197th mini batch complete. LOSS: 4.164542, ACCR: 0.468750\n",
      " 198th mini batch complete. LOSS: 4.181949, ACCR: 0.453125\n",
      " 199th mini batch complete. LOSS: 4.118116, ACCR: 0.515625\n",
      " 200th mini batch complete. LOSS: 4.071621, ACCR: 0.562500\n",
      "Testing... LOSS: 4.243289, ACCR: 0.389830\n",
      " 201th mini batch complete. LOSS: 4.102787, ACCR: 0.531250\n",
      " 202th mini batch complete. LOSS: 4.039788, ACCR: 0.593750\n",
      " 203th mini batch complete. LOSS: 4.039804, ACCR: 0.593750\n",
      " 204th mini batch complete. LOSS: 4.164804, ACCR: 0.468750\n",
      " 205th mini batch complete. LOSS: 4.102258, ACCR: 0.531250\n",
      " 206th mini batch complete. LOSS: 4.088367, ACCR: 0.546875\n",
      " 207th mini batch complete. LOSS: 4.102155, ACCR: 0.531250\n",
      " 208th mini batch complete. LOSS: 4.119094, ACCR: 0.515625\n",
      " 209th mini batch complete. LOSS: 4.103403, ACCR: 0.531250\n",
      " 210th mini batch complete. LOSS: 4.102602, ACCR: 0.531250\n",
      " 211th mini batch complete. LOSS: 4.055830, ACCR: 0.578125\n",
      " 212th mini batch complete. LOSS: 4.119116, ACCR: 0.515625\n",
      " 213th mini batch complete. LOSS: 4.164970, ACCR: 0.468750\n",
      " 214th mini batch complete. LOSS: 4.164747, ACCR: 0.468750\n",
      " 215th mini batch complete. LOSS: 4.133208, ACCR: 0.500000\n",
      " 216th mini batch complete. LOSS: 4.102144, ACCR: 0.531250\n",
      " 217th mini batch complete. LOSS: 4.103011, ACCR: 0.531250\n",
      " 218th mini batch complete. LOSS: 4.211252, ACCR: 0.421875\n",
      " 219th mini batch complete. LOSS: 4.227651, ACCR: 0.406250\n",
      " 220th mini batch complete. LOSS: 4.180010, ACCR: 0.453125\n",
      " 221th mini batch complete. LOSS: 4.024748, ACCR: 0.609375\n",
      " 222th mini batch complete. LOSS: 4.259994, ACCR: 0.375000\n",
      " 223th mini batch complete. LOSS: 4.086840, ACCR: 0.546875\n",
      " 224th mini batch complete. LOSS: 4.086794, ACCR: 0.546875\n",
      " 225th mini batch complete. LOSS: 4.117731, ACCR: 0.515625\n",
      " 226th mini batch complete. LOSS: 4.289513, ACCR: 0.343750\n",
      " 227th mini batch complete. LOSS: 4.180085, ACCR: 0.453125\n",
      " 228th mini batch complete. LOSS: 4.056512, ACCR: 0.578125\n",
      " 229th mini batch complete. LOSS: 4.211728, ACCR: 0.421875\n",
      " 230th mini batch complete. LOSS: 4.195848, ACCR: 0.437500\n",
      " 231th mini batch complete. LOSS: 3.978333, ACCR: 0.656250\n",
      " 232th mini batch complete. LOSS: 4.121070, ACCR: 0.515625\n",
      " 233th mini batch complete. LOSS: 4.055840, ACCR: 0.578125\n",
      " 234th mini batch complete. LOSS: 4.055907, ACCR: 0.578125\n",
      " 235th mini batch complete. LOSS: 4.164487, ACCR: 0.468750\n",
      " 236th mini batch complete. LOSS: 4.070952, ACCR: 0.562500\n",
      " 237th mini batch complete. LOSS: 4.039341, ACCR: 0.593750\n",
      " 238th mini batch complete. LOSS: 4.071163, ACCR: 0.562500\n",
      " 239th mini batch complete. LOSS: 4.072232, ACCR: 0.562500\n",
      " 240th mini batch complete. LOSS: 4.086878, ACCR: 0.546875\n",
      " 241th mini batch complete. LOSS: 4.164432, ACCR: 0.468750\n",
      " 242th mini batch complete. LOSS: 4.211250, ACCR: 0.421875\n",
      " 243th mini batch complete. LOSS: 4.226820, ACCR: 0.406250\n",
      " 244th mini batch complete. LOSS: 4.164845, ACCR: 0.468750\n",
      " 245th mini batch complete. LOSS: 4.086583, ACCR: 0.546875\n",
      " 246th mini batch complete. LOSS: 4.102016, ACCR: 0.531250\n",
      " 247th mini batch complete. LOSS: 4.164424, ACCR: 0.468750\n",
      " 248th mini batch complete. LOSS: 4.055634, ACCR: 0.578125\n",
      " 249th mini batch complete. LOSS: 4.071449, ACCR: 0.562500\n",
      " 250th mini batch complete. LOSS: 4.070807, ACCR: 0.562500\n",
      " 251th mini batch complete. LOSS: 3.995900, ACCR: 0.640625\n",
      " 252th mini batch complete. LOSS: 4.103439, ACCR: 0.531250\n",
      " 253th mini batch complete. LOSS: 4.008327, ACCR: 0.625000\n",
      " 254th mini batch complete. LOSS: 4.102212, ACCR: 0.531250\n",
      " 255th mini batch complete. LOSS: 4.117429, ACCR: 0.515625\n",
      " 256th mini batch complete. LOSS: 4.117733, ACCR: 0.515625\n",
      " 257th mini batch complete. LOSS: 4.071185, ACCR: 0.562500\n",
      " 258th mini batch complete. LOSS: 4.055440, ACCR: 0.578125\n",
      " 259th mini batch complete. LOSS: 3.992714, ACCR: 0.640625\n",
      " 260th mini batch complete. LOSS: 4.117738, ACCR: 0.515625\n",
      " 261th mini batch complete. LOSS: 4.039629, ACCR: 0.593750\n",
      " 262th mini batch complete. LOSS: 4.148907, ACCR: 0.484375\n",
      " 263th mini batch complete. LOSS: 4.117442, ACCR: 0.515625\n",
      " 264th mini batch complete. LOSS: 4.117723, ACCR: 0.515625\n",
      " 265th mini batch complete. LOSS: 4.070601, ACCR: 0.562500\n",
      " 266th mini batch complete. LOSS: 4.101868, ACCR: 0.531250\n",
      " 267th mini batch complete. LOSS: 4.055247, ACCR: 0.578125\n",
      " 268th mini batch complete. LOSS: 3.994238, ACCR: 0.640625\n",
      " 269th mini batch complete. LOSS: 4.117700, ACCR: 0.515625\n",
      " 270th mini batch complete. LOSS: 4.133461, ACCR: 0.500000\n",
      " 271th mini batch complete. LOSS: 4.086182, ACCR: 0.546875\n",
      " 272th mini batch complete. LOSS: 4.179987, ACCR: 0.453125\n",
      " 273th mini batch complete. LOSS: 4.086175, ACCR: 0.546875\n",
      " 274th mini batch complete. LOSS: 4.086380, ACCR: 0.546875\n",
      " 275th mini batch complete. LOSS: 3.992622, ACCR: 0.640625\n",
      " 276th mini batch complete. LOSS: 4.118013, ACCR: 0.515625\n",
      " 277th mini batch complete. LOSS: 4.086505, ACCR: 0.546875\n",
      " 278th mini batch complete. LOSS: 4.117618, ACCR: 0.515625\n",
      " 279th mini batch complete. LOSS: 4.086332, ACCR: 0.546875\n",
      " 280th mini batch complete. LOSS: 4.132962, ACCR: 0.500000\n",
      " 281th mini batch complete. LOSS: 4.054953, ACCR: 0.578125\n",
      " 282th mini batch complete. LOSS: 4.086923, ACCR: 0.546875\n",
      " 283th mini batch complete. LOSS: 4.039598, ACCR: 0.593750\n",
      " 284th mini batch complete. LOSS: 4.039759, ACCR: 0.593750\n",
      " 285th mini batch complete. LOSS: 4.181819, ACCR: 0.453125\n",
      " 286th mini batch complete. LOSS: 4.170778, ACCR: 0.468750\n",
      " 287th mini batch complete. LOSS: 4.086171, ACCR: 0.546875\n",
      " 288th mini batch complete. LOSS: 4.055347, ACCR: 0.578125\n",
      " 289th mini batch complete. LOSS: 4.133329, ACCR: 0.500000\n",
      " 290th mini batch complete. LOSS: 4.070493, ACCR: 0.562500\n",
      " 291th mini batch complete. LOSS: 4.149137, ACCR: 0.484375\n",
      " 292th mini batch complete. LOSS: 4.257975, ACCR: 0.375000\n",
      " 293th mini batch complete. LOSS: 4.070838, ACCR: 0.562500\n",
      " 294th mini batch complete. LOSS: 4.054808, ACCR: 0.578125\n",
      " 295th mini batch complete. LOSS: 4.133103, ACCR: 0.500000\n",
      " 296th mini batch complete. LOSS: 4.054978, ACCR: 0.578125\n",
      " 297th mini batch complete. LOSS: 4.132993, ACCR: 0.500000\n",
      " 298th mini batch complete. LOSS: 4.086179, ACCR: 0.546875\n",
      " 299th mini batch complete. LOSS: 4.117362, ACCR: 0.515625\n",
      " 300th mini batch complete. LOSS: 4.039510, ACCR: 0.593750\n",
      "Testing... LOSS: 4.243064, ACCR: 0.389830\n",
      " 301th mini batch complete. LOSS: 4.164309, ACCR: 0.468750\n",
      " 302th mini batch complete. LOSS: 4.133129, ACCR: 0.500000\n",
      " 303th mini batch complete. LOSS: 4.196109, ACCR: 0.437500\n",
      " 304th mini batch complete. LOSS: 4.039485, ACCR: 0.593750\n",
      " 305th mini batch complete. LOSS: 4.152916, ACCR: 0.480000\n",
      "Start 2th epoch.\n",
      "   1th mini batch complete. LOSS: 4.117751, ACCR: 0.515625\n",
      "   2th mini batch complete. LOSS: 4.195470, ACCR: 0.437500\n",
      "   3th mini batch complete. LOSS: 4.132913, ACCR: 0.500000\n",
      "   4th mini batch complete. LOSS: 4.024004, ACCR: 0.609375\n",
      "   5th mini batch complete. LOSS: 4.101836, ACCR: 0.531250\n",
      "   6th mini batch complete. LOSS: 4.117377, ACCR: 0.515625\n",
      "   7th mini batch complete. LOSS: 4.180094, ACCR: 0.453125\n",
      "   8th mini batch complete. LOSS: 4.211602, ACCR: 0.421875\n",
      "   9th mini batch complete. LOSS: 4.117577, ACCR: 0.515625\n",
      "  10th mini batch complete. LOSS: 4.086124, ACCR: 0.546875\n",
      "  11th mini batch complete. LOSS: 4.133008, ACCR: 0.500000\n",
      "  12th mini batch complete. LOSS: 4.039948, ACCR: 0.593750\n",
      "  13th mini batch complete. LOSS: 4.148630, ACCR: 0.484375\n",
      "  14th mini batch complete. LOSS: 4.164613, ACCR: 0.468750\n",
      "  15th mini batch complete. LOSS: 3.992546, ACCR: 0.640625\n",
      "  16th mini batch complete. LOSS: 4.070676, ACCR: 0.562500\n",
      "  17th mini batch complete. LOSS: 4.117725, ACCR: 0.515625\n",
      "  18th mini batch complete. LOSS: 4.148595, ACCR: 0.484375\n",
      "  19th mini batch complete. LOSS: 4.179976, ACCR: 0.453125\n",
      "  20th mini batch complete. LOSS: 4.180099, ACCR: 0.453125\n",
      "  21th mini batch complete. LOSS: 3.992606, ACCR: 0.640625\n",
      "  22th mini batch complete. LOSS: 4.102666, ACCR: 0.531250\n",
      "  23th mini batch complete. LOSS: 4.008182, ACCR: 0.625000\n",
      "  24th mini batch complete. LOSS: 4.070815, ACCR: 0.562500\n",
      "  25th mini batch complete. LOSS: 4.179958, ACCR: 0.453125\n",
      "  26th mini batch complete. LOSS: 4.070607, ACCR: 0.562500\n",
      "  27th mini batch complete. LOSS: 4.071191, ACCR: 0.562500\n",
      "  28th mini batch complete. LOSS: 4.119728, ACCR: 0.515625\n",
      "  29th mini batch complete. LOSS: 4.102029, ACCR: 0.531250\n",
      "  30th mini batch complete. LOSS: 4.007946, ACCR: 0.625000\n",
      "  31th mini batch complete. LOSS: 3.992534, ACCR: 0.640625\n",
      "  32th mini batch complete. LOSS: 4.070454, ACCR: 0.562500\n",
      "  33th mini batch complete. LOSS: 4.101868, ACCR: 0.531250\n",
      "  34th mini batch complete. LOSS: 4.180295, ACCR: 0.453125\n",
      "  35th mini batch complete. LOSS: 4.133159, ACCR: 0.500000\n",
      "  36th mini batch complete. LOSS: 4.039402, ACCR: 0.593750\n",
      "  37th mini batch complete. LOSS: 4.024421, ACCR: 0.609375\n",
      "  38th mini batch complete. LOSS: 4.039549, ACCR: 0.593750\n",
      "  39th mini batch complete. LOSS: 4.102048, ACCR: 0.531250\n",
      "  40th mini batch complete. LOSS: 4.148708, ACCR: 0.484375\n",
      "  41th mini batch complete. LOSS: 4.117383, ACCR: 0.515625\n",
      "  42th mini batch complete. LOSS: 4.054826, ACCR: 0.578125\n",
      "  43th mini batch complete. LOSS: 4.101688, ACCR: 0.531250\n",
      "  44th mini batch complete. LOSS: 4.133280, ACCR: 0.500000\n",
      "  45th mini batch complete. LOSS: 4.023868, ACCR: 0.609375\n",
      "  46th mini batch complete. LOSS: 4.195385, ACCR: 0.437500\n",
      "  47th mini batch complete. LOSS: 4.132897, ACCR: 0.500000\n",
      "  48th mini batch complete. LOSS: 4.257967, ACCR: 0.375000\n",
      "  49th mini batch complete. LOSS: 4.086206, ACCR: 0.546875\n",
      "  50th mini batch complete. LOSS: 4.070509, ACCR: 0.562500\n",
      "  51th mini batch complete. LOSS: 4.132940, ACCR: 0.500000\n",
      "  52th mini batch complete. LOSS: 4.054879, ACCR: 0.578125\n",
      "  53th mini batch complete. LOSS: 4.133038, ACCR: 0.500000\n",
      "  54th mini batch complete. LOSS: 4.039348, ACCR: 0.593750\n",
      "  55th mini batch complete. LOSS: 3.961335, ACCR: 0.671875\n",
      "  56th mini batch complete. LOSS: 4.148598, ACCR: 0.484375\n",
      "  57th mini batch complete. LOSS: 4.148697, ACCR: 0.484375\n",
      "  58th mini batch complete. LOSS: 4.148703, ACCR: 0.484375\n",
      "  59th mini batch complete. LOSS: 4.227046, ACCR: 0.406250\n",
      "  60th mini batch complete. LOSS: 4.039378, ACCR: 0.593750\n",
      "  61th mini batch complete. LOSS: 4.242640, ACCR: 0.390625\n",
      "  62th mini batch complete. LOSS: 4.180081, ACCR: 0.453125\n",
      "  63th mini batch complete. LOSS: 4.101714, ACCR: 0.531250\n",
      "  64th mini batch complete. LOSS: 4.179796, ACCR: 0.453125\n",
      "  65th mini batch complete. LOSS: 4.164183, ACCR: 0.468750\n",
      "  66th mini batch complete. LOSS: 4.132896, ACCR: 0.500000\n",
      "  67th mini batch complete. LOSS: 4.070959, ACCR: 0.562500\n",
      "  68th mini batch complete. LOSS: 4.070536, ACCR: 0.562500\n",
      "  69th mini batch complete. LOSS: 4.148735, ACCR: 0.484375\n",
      "  70th mini batch complete. LOSS: 4.086380, ACCR: 0.546875\n",
      "  71th mini batch complete. LOSS: 4.070707, ACCR: 0.562500\n",
      "  72th mini batch complete. LOSS: 4.117488, ACCR: 0.515625\n",
      "  73th mini batch complete. LOSS: 4.054796, ACCR: 0.578125\n",
      "  74th mini batch complete. LOSS: 4.070716, ACCR: 0.562500\n",
      "  75th mini batch complete. LOSS: 4.148586, ACCR: 0.484375\n",
      "  76th mini batch complete. LOSS: 4.070525, ACCR: 0.562500\n",
      "  77th mini batch complete. LOSS: 4.101714, ACCR: 0.531250\n",
      "  78th mini batch complete. LOSS: 4.242289, ACCR: 0.390625\n",
      "  79th mini batch complete. LOSS: 4.070425, ACCR: 0.562500\n",
      "  80th mini batch complete. LOSS: 4.086177, ACCR: 0.546875\n",
      "  81th mini batch complete. LOSS: 4.086001, ACCR: 0.546875\n",
      "  82th mini batch complete. LOSS: 4.164280, ACCR: 0.468750\n",
      "  83th mini batch complete. LOSS: 4.070747, ACCR: 0.562500\n",
      "  84th mini batch complete. LOSS: 4.086133, ACCR: 0.546875\n",
      "  85th mini batch complete. LOSS: 4.070642, ACCR: 0.562500\n",
      "  86th mini batch complete. LOSS: 4.008073, ACCR: 0.625000\n",
      "  87th mini batch complete. LOSS: 4.117521, ACCR: 0.515625\n",
      "  88th mini batch complete. LOSS: 4.055054, ACCR: 0.578125\n",
      "  89th mini batch complete. LOSS: 4.132990, ACCR: 0.500000\n",
      "  90th mini batch complete. LOSS: 4.242398, ACCR: 0.390625\n",
      "  91th mini batch complete. LOSS: 4.039285, ACCR: 0.593750\n",
      "  92th mini batch complete. LOSS: 4.179757, ACCR: 0.453125\n",
      "  93th mini batch complete. LOSS: 4.117457, ACCR: 0.515625\n",
      "  94th mini batch complete. LOSS: 4.101607, ACCR: 0.531250\n",
      "  95th mini batch complete. LOSS: 4.054830, ACCR: 0.578125\n",
      "Testing... LOSS: 4.242995, ACCR: 0.389830\n",
      "  96th mini batch complete. LOSS: 4.117336, ACCR: 0.515625\n",
      "  97th mini batch complete. LOSS: 4.117233, ACCR: 0.515625\n",
      "  98th mini batch complete. LOSS: 4.117244, ACCR: 0.515625\n",
      "  99th mini batch complete. LOSS: 3.976916, ACCR: 0.656250\n",
      " 100th mini batch complete. LOSS: 4.211015, ACCR: 0.421875\n",
      " 101th mini batch complete. LOSS: 4.211213, ACCR: 0.421875\n",
      " 102th mini batch complete. LOSS: 4.179718, ACCR: 0.453125\n",
      " 103th mini batch complete. LOSS: 4.226868, ACCR: 0.406250\n",
      " 104th mini batch complete. LOSS: 4.211068, ACCR: 0.421875\n",
      " 105th mini batch complete. LOSS: 4.054985, ACCR: 0.578125\n",
      " 106th mini batch complete. LOSS: 4.054837, ACCR: 0.578125\n",
      " 107th mini batch complete. LOSS: 4.007896, ACCR: 0.625000\n",
      " 108th mini batch complete. LOSS: 4.179745, ACCR: 0.453125\n",
      " 109th mini batch complete. LOSS: 4.086244, ACCR: 0.546875\n",
      " 110th mini batch complete. LOSS: 4.148514, ACCR: 0.484375\n",
      " 111th mini batch complete. LOSS: 4.148998, ACCR: 0.484375\n",
      " 112th mini batch complete. LOSS: 4.086048, ACCR: 0.546875\n",
      " 113th mini batch complete. LOSS: 4.070439, ACCR: 0.562500\n",
      " 114th mini batch complete. LOSS: 4.007907, ACCR: 0.625000\n",
      " 115th mini batch complete. LOSS: 4.023642, ACCR: 0.609375\n",
      " 116th mini batch complete. LOSS: 4.117251, ACCR: 0.515625\n",
      " 117th mini batch complete. LOSS: 4.133188, ACCR: 0.500000\n",
      " 118th mini batch complete. LOSS: 4.101671, ACCR: 0.531250\n",
      " 119th mini batch complete. LOSS: 4.179954, ACCR: 0.453125\n",
      " 120th mini batch complete. LOSS: 4.101642, ACCR: 0.531250\n",
      " 121th mini batch complete. LOSS: 4.195482, ACCR: 0.437500\n",
      " 122th mini batch complete. LOSS: 4.164111, ACCR: 0.468750\n",
      " 123th mini batch complete. LOSS: 4.117273, ACCR: 0.515625\n",
      " 124th mini batch complete. LOSS: 4.164112, ACCR: 0.468750\n",
      " 125th mini batch complete. LOSS: 4.195432, ACCR: 0.437500\n",
      " 126th mini batch complete. LOSS: 4.195370, ACCR: 0.437500\n",
      " 127th mini batch complete. LOSS: 4.101766, ACCR: 0.531250\n",
      " 128th mini batch complete. LOSS: 4.164224, ACCR: 0.468750\n",
      " 129th mini batch complete. LOSS: 4.164134, ACCR: 0.468750\n",
      " 130th mini batch complete. LOSS: 4.117290, ACCR: 0.515625\n",
      " 131th mini batch complete. LOSS: 4.087397, ACCR: 0.546875\n",
      " 132th mini batch complete. LOSS: 4.148566, ACCR: 0.484375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#x, y, d = data_helper.load_data_and_labels_and_dictionaries()\n",
    "\n",
    "# Split original data into two groups for training and testing.\n",
    "train_x = data[:train_size]\n",
    "train_y = one_hot_data[:train_size]\n",
    "test_x = data[train_size:]\n",
    "test_y = one_hot_data[train_size:]\n",
    "NUM_CLASSES = 101\n",
    "\n",
    "NUM_TESTS         = 2000\n",
    "NUM_EPOCHS        = 10\n",
    "NUM_MINI_BATCH    = 64\n",
    "EMBEDDING_SIZE    = 128\n",
    "NUM_FILTERS       = 128\n",
    "FILTER_SIZES      = [ 3, 4, 5 ]\n",
    "L2_LAMBDA         = 0.0001\n",
    "EVALUATE_EVERY    = 100\n",
    "CHECKPOINTS_EVERY = 1000\n",
    "SUMMARY_LOG_DIR = \"tmp/tensorflow_log\"\n",
    "CHECKPOINTS_DIR = 'checkpoints'\n",
    "\n",
    "keep = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_x = tf.placeholder(tf.int32, [None, max_size])\n",
    "input_y = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "\n",
    "\n",
    "with tf.name_scope('embedding'):\n",
    "    w  = tf.Variable(tf.random_uniform([len(dictionaries), EMBEDDING_SIZE], -1.0, 1.0), name='weight')\n",
    "    e  = tf.nn.embedding_lookup(w, input_x)\n",
    "    ex = tf.expand_dims(e, -1)\n",
    "\n",
    "# Define 3rd and 4th layer (Temporal 1-D convolutional and max-pooling layer).\n",
    "p_array = []\n",
    "for filter_size in FILTER_SIZES:\n",
    "    with tf.name_scope('conv-%d' % filter_size):\n",
    "        w  = tf.Variable(tf.truncated_normal([ filter_size, EMBEDDING_SIZE, 1, NUM_FILTERS ], stddev=0.02), name='weight')\n",
    "        b  = tf.Variable(tf.constant(0.1, shape=[ NUM_FILTERS ]), name='bias')\n",
    "        c0 = tf.nn.conv2d(ex, w, [ 1, 1, 1, 1 ], 'VALID')\n",
    "        c1 = tf.nn.relu(tf.nn.bias_add(c0, b))\n",
    "        c2 = tf.nn.max_pool(c1, [ 1,  max_size - filter_size + 1, 1, 1 ], [ 1, 1, 1, 1 ], 'VALID')\n",
    "        p_array.append(c2)\n",
    "\n",
    "p = tf.concat(3, p_array)\n",
    "\n",
    "\n",
    "with tf.name_scope('fc'):\n",
    "    total_filters = NUM_FILTERS * len(FILTER_SIZES)\n",
    "    w = tf.Variable(tf.truncated_normal([ total_filters, NUM_CLASSES ], stddev=0.02), name='weight')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[ NUM_CLASSES ]), name='bias')\n",
    "    h0 = tf.nn.dropout(tf.reshape(p, [ -1, total_filters ]), keep)\n",
    "    predict_y = tf.nn.softmax(tf.matmul(h0, w) + b)\n",
    "\n",
    "\n",
    "xentropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predict_y, input_y))\n",
    "loss = xentropy + L2_LAMBDA * tf.nn.l2_loss(w)\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "train = tf.train.AdamOptimizer(0.0001).minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "\n",
    "predict  = tf.equal(tf.argmax(predict_y, 1), tf.argmax(input_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n",
    "\n",
    "loss_sum   = tf.scalar_summary('train loss', loss)\n",
    "accr_sum   = tf.scalar_summary('train accuracy', accuracy)\n",
    "t_loss_sum = tf.scalar_summary('general loss', loss)\n",
    "t_accr_sum = tf.scalar_summary('general accuracy', accuracy)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    writer = tf.train.SummaryWriter(SUMMARY_LOG_DIR, sess.graph)\n",
    "\n",
    "    train_x_length = len(train_x)\n",
    "    batch_count = int(train_x_length / NUM_MINI_BATCH) + 1\n",
    "\n",
    "    print('Start training.')\n",
    "    print('     epoch: %d' % NUM_EPOCHS)\n",
    "    print('mini batch: %d' % NUM_MINI_BATCH)\n",
    "    print('train data: %d' % train_x_length)\n",
    "    print(' test data: %d' % len(test_x))\n",
    "    print('We will loop %d count per an epoch.' % batch_count)\n",
    "\n",
    "   \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        random_indice = np.random.permutation(train_x_length)\n",
    "        print('Start %dth epoch.' % (epoch + 1))\n",
    "        for i in range(batch_count):\n",
    "            mini_batch_x = []\n",
    "            mini_batch_y = []\n",
    "            for j in range(min(train_x_length - i * NUM_MINI_BATCH, NUM_MINI_BATCH)):\n",
    "                mini_batch_x.append(train_x[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "                mini_batch_y.append(train_y[random_indice[i * NUM_MINI_BATCH + j]])\n",
    "\n",
    "            \n",
    "            _, v1, v2, v3, v4 = sess.run(\n",
    "                [ train, loss, accuracy, loss_sum, accr_sum ],\n",
    "                feed_dict={ input_x: mini_batch_x, input_y: mini_batch_y, keep: 0.5 }\n",
    "            )\n",
    "            print('%4dth mini batch complete. LOSS: %f, ACCR: %f' % (i + 1, v1, v2))\n",
    "\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            writer.add_summary(v3, current_step)\n",
    "            writer.add_summary(v4, current_step)\n",
    "\n",
    "\n",
    "            if current_step % CHECKPOINTS_EVERY == 0:\n",
    "                saver.save(sess, CHECKPOINTS_DIR + '/model', global_step=current_step)\n",
    "                print('Checkout was completed.')\n",
    "\n",
    " \n",
    "            if current_step % EVALUATE_EVERY == 0:\n",
    "                #random_test_indice = np.random.permutation(100)\n",
    "                #random_test_x = test_x[int(random_test_indice)]\n",
    "                #random_test_y = test_y[int(random_test_indice)]\n",
    "\n",
    "                v1, v2, v3, v4 = sess.run(\n",
    "                    [ loss, accuracy, t_loss_sum, t_accr_sum ],\n",
    "                    feed_dict={ input_x: test_x, input_y: test_y, keep: 1.0 }\n",
    "                )\n",
    "                print('Testing... LOSS: %f, ACCR: %f' % (v1, v2))\n",
    "                writer.add_summary(v3, current_step)\n",
    "                writer.add_summary(v4, current_step)\n",
    "\n",
    "\n",
    "    saver.save(sess, CHECKPOINTS_DIR + '/model-last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (3,) and (4,) are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m           \u001b[0mnew_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    107\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\"\n\u001b[0;32m--> 108\u001b[0;31m                        % (self, other))\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3 and 4 are not compatible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0msupervisor_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"supervisor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupervisor_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_ph)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2553199840fc>\u001b[0m in \u001b[0;36munpack_sequence\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   2487\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m   \"\"\"\n\u001b[0;32m-> 2489\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    701\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    702\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    704\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2310\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1703\u001b[0m                          % op.type)\n\u001b[0;32m-> 1704\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_TransposeShape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2117\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m   transpose_shape = op.inputs[1].get_shape().merge_with(tensor_shape.vector(\n\u001b[0;32m-> 2119\u001b[0;31m       input_shape.ndims))\n\u001b[0m\u001b[1;32m   2120\u001b[0m   \u001b[0mtranspose_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         raise ValueError(\"Shapes %s and %s are not compatible\" %\n\u001b[0;32m--> 570\u001b[0;31m                          (self, other))\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (3,) and (4,) are not compatible"
     ]
    }
   ],
   "source": [
    "!rm -rf tmp/tensorflow_log/*\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "num_of_input_nodes = 1\n",
    "num_of_hidden_nodes = 101\n",
    "num_of_output_nodes = 1\n",
    "num_of_training_epochs = 50000\n",
    "batch_size = 100\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.001\n",
    "forget_bias = 0.9\n",
    "num_of_sample = 1000\n",
    "num_layers = 1\n",
    "\n",
    "batch_size = 100\n",
    "sequences_length = 30 \n",
    "test_num = int(num_of_sample*0.3)\n",
    "class_num = 101\n",
    "\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128 \n",
    "\n",
    "\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def create_batch(batch_size, X, t):\n",
    "    #X = X.as_matrix()\n",
    "    #t = t.as_matrix()\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def unpack_sequence(tensor):\n",
    "    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def pack_sequence(sequence):\n",
    "    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "def inference(input_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        in_size = num_of_hidden_nodes\n",
    "        out_size = class_num\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocabulary_size, embedding_size])\n",
    "        \n",
    "       \n",
    "        # network = tf.nn.rnn_cell.LSTMCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.GRUCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(network, output_keep_prob=0.5)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * num_layers)\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_ph)\n",
    "        inputs =  unpack_sequence(inputs)\n",
    "       \n",
    "        rnn_output, states_op = tf.nn.rnn(network,inputs,dtype=tf.int32)\n",
    "        #rnn_output, states_op = tf.nn.dynamic_rnn(network,inputs,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        #rnn_output = pack_sequence(rnn_output)\n",
    "        #state_op = pack_sequence(states_op)\n",
    "        \n",
    "        output_op = tf.transpose(rnn_output, [1, 0, 2])\n",
    "        #output_op = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        #output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight) + bias)\n",
    "\n",
    " \n",
    "        tf.histogram_summary(\"weights\", weight)\n",
    "        tf.histogram_summary(\"biases\", bias)\n",
    "        tf.histogram_summary(\"output\",  output_op)\n",
    "        results = [weight, bias]\n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss_op = - tf.reduce_sum(supervisor_ph * tf.log(output_op))\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "def accuracy(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(output_op,1), tf.argmax(supervisor_ph,1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "        return accuracy_op\n",
    "\n",
    "def calc_accuracy(accuracy_opp, X, t):\n",
    "    inputs, targets = create_batch(len(X), X, t)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: targets\n",
    "    }\n",
    "    accurecy = sess.run(accuracy_op, feed_dict=pred_dict)\n",
    "    print(accurecy)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.int32, [None, sequences_length, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, class_num], name=\"supervisor\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "    accuracy_op = accuracy(output_op, supervisor_ph)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            #inputs, supervisors = create_batch(batch_size, training_predictors_tf , training_classes_tf)\n",
    "            inputs, supervisors = training_predictors_tf , training_classes_tf\n",
    "            train_dict = {\n",
    "                input_ph:   inputs,\n",
    "                supervisor_ph: supervisors\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 1000 == 0:\n",
    "                #summary_str, loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                loss = sess.run(loss_op, feed_dict=train_dict)\n",
    "                print(\"train#{}, loss: {}\".format(epoch, loss))\n",
    "                #summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 5000 == 0:\n",
    "                    calc_accuracy(output_op, test_predictors_tf, test_classes_tf)\n",
    "        calc_accuracy(output_op, X_test, t_test)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
