{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>icon_url</th>\n",
       "      <th>id</th>\n",
       "      <th>items_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>45</td>\n",
       "      <td>None</td>\n",
       "      <td>画像認識</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     followers_count icon_url    id  items_count\n",
       "685               45     None  画像認識           79"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"tags.json\")\n",
    "df[df[\"id\"]== \"画像認識\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('qiita2')\n",
    "table.item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = table.scan()\n",
    "data = response['Items']\n",
    "\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    data.extend(response['Items'])\n",
    "articles = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at_in_words\n",
      "tags\n",
      "created_at\n",
      "url\n",
      "gist_url\n",
      "stocked\n",
      "title\n",
      "tweet\n",
      "updated_at\n",
      "id\n",
      "updated_at_in_words\n",
      "private\n",
      "comment_count\n",
      "created_at_as_seconds\n",
      "body\n",
      "user\n",
      "stock_count\n",
      "stock_users\n",
      "uuid\n",
      "raw_body\n"
     ]
    }
   ],
   "source": [
    "for k in articles[0][\"json\"]:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/9068c4ec90f73848c4c0cfeb25eb082d20484587/medium.jpg?1465035225',\n",
       "  'name': 'HTML',\n",
       "  'url_name': 'html',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/b4438ce0a78810256791a266e87c74a76b555de1/medium.jpg?1419699326',\n",
       "  'name': 'CSS',\n",
       "  'url_name': 'css',\n",
       "  'versions': []},\n",
       " {'following': False,\n",
       "  'icon_url': 'https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/12f0e530959eabdee68c2ba3bb0052a6f9a23283/medium.jpg?1430329273',\n",
       "  'name': 'bootstrap',\n",
       "  'url_name': 'bootstrap',\n",
       "  'versions': []}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0][\"json\"][\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5791"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_data = []\n",
    "tag_data = []\n",
    "for a in articles:\n",
    "    json = a[\"json\"]\n",
    "    body = json[\"title\"]\n",
    "    tags = json[\"tags\"]\n",
    "    for tag in tags:        \n",
    "        body_data.append(body)\n",
    "        tag_data.append(tag[\"name\"])\n",
    "\n",
    "len(body_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers_count</th>\n",
       "      <th>icon_url</th>\n",
       "      <th>id</th>\n",
       "      <th>items_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17743</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Ruby</td>\n",
       "      <td>12523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30279</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td>12357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16438</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>Python</td>\n",
       "      <td>8890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13196</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>iOS</td>\n",
       "      <td>8547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19287</td>\n",
       "      <td>https://s3-ap-northeast-1.amazonaws.com/qiita-...</td>\n",
       "      <td>PHP</td>\n",
       "      <td>8233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   followers_count                                           icon_url  \\\n",
       "0            17743  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "1            30279  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "2            16438  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "3            13196  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "4            19287  https://s3-ap-northeast-1.amazonaws.com/qiita-...   \n",
       "\n",
       "           id  items_count  \n",
       "0        Ruby        12523  \n",
       "1  JavaScript        12357  \n",
       "2      Python         8890  \n",
       "3         iOS         8547  \n",
       "4         PHP         8233  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"tags.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list = df.ix[:,\"id\"].tolist()[:100]\n",
    "name_list.append(\"other\")\n",
    "one_hot = pd.get_dummies(name_list)\n",
    "one_hot.ix[:,\"Ruby\"]\n",
    "one_hot.ix[:,\"Ruby\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5791"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data = []\n",
    "for t in tag_data:\n",
    "    if t not in name_list:\n",
    "        t = \"other\"\n",
    "    d = one_hot.ix[:,t]\n",
    "    one_hot_data.append(d)\n",
    "len(one_hot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4632\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(one_hot_data)*0.8)\n",
    "print(train_size)\n",
    "training_predictors_tf = body_data[:train_size]\n",
    "test_predictors_tf = body_data[train_size:]\n",
    "training_classes_tf = one_hot_data[:train_size]\n",
    "test_classes_tf = one_hot_data[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (3,) and (4,) are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m           \u001b[0mnew_dims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    107\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\"\n\u001b[0;32m--> 108\u001b[0;31m                        % (self, other))\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 3 and 4 are not compatible",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2553199840fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0msupervisor_ph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"supervisor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mloss_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupervisor_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mtraining_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-2553199840fc>\u001b[0m in \u001b[0;36minference\u001b[0;34m(input_ph)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-2553199840fc>\u001b[0m in \u001b[0;36munpack_sequence\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   2487\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m   \"\"\"\n\u001b[0;32m-> 2489\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    701\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    702\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                            op_def=op_def)\n\u001b[0m\u001b[1;32m    704\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2310\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[1;32m   1703\u001b[0m                          % op.type)\n\u001b[0;32m-> 1704\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_TransposeShape\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2117\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m   transpose_shape = op.inputs[1].get_shape().merge_with(tensor_shape.vector(\n\u001b[0;32m-> 2119\u001b[0;31m       input_shape.ndims))\n\u001b[0m\u001b[1;32m   2120\u001b[0m   \u001b[0mtranspose_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtranspose_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tamotoyoshifumi/anaconda2/envs/py3/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mmerge_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    568\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         raise ValueError(\"Shapes %s and %s are not compatible\" %\n\u001b[0;32m--> 570\u001b[0;31m                          (self, other))\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (3,) and (4,) are not compatible"
     ]
    }
   ],
   "source": [
    "!rm -rf tmp/tensorflow_log/*\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "num_of_input_nodes = 1\n",
    "num_of_hidden_nodes = 101\n",
    "num_of_output_nodes = 1\n",
    "num_of_training_epochs = 50000\n",
    "batch_size = 100\n",
    "num_of_prediction_epochs = 100\n",
    "learning_rate = 0.001\n",
    "forget_bias = 0.9\n",
    "num_of_sample = 1000\n",
    "num_layers = 1\n",
    "\n",
    "batch_size = 100\n",
    "sequences_length = 30 \n",
    "test_num = int(num_of_sample*0.3)\n",
    "class_num = 101\n",
    "\n",
    "vocabulary_size = 50000\n",
    "embedding_size = 128 \n",
    "\n",
    "\n",
    "def get_batch(batch_size, X, t):\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def create_batch(batch_size, X, t):\n",
    "    #X = X.as_matrix()\n",
    "    #t = t.as_matrix()\n",
    "    rnum = [random.randint(0, len(X) - 1) for x in range(batch_size)]\n",
    "    xs = np.array([[[y] for y in list(X[r])] for r in rnum])\n",
    "    ts = np.array([t[r] for r in rnum])\n",
    "    return xs, ts\n",
    "\n",
    "\n",
    "def unpack_sequence(tensor):\n",
    "    return tf.unpack(tf.transpose(tensor, perm=[1, 0, 2]))\n",
    "\n",
    "def pack_sequence(sequence):\n",
    "    return tf.transpose(tf.pack(sequence), perm=[1, 0, 2])\n",
    "\n",
    "def inference(input_ph):\n",
    "    with tf.name_scope(\"inference\") as scope:\n",
    "        in_size = num_of_hidden_nodes\n",
    "        out_size = class_num\n",
    "        weight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocabulary_size, embedding_size])\n",
    "        \n",
    "       \n",
    "        # network = tf.nn.rnn_cell.LSTMCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.GRUCell(num_of_hidden_nodes)\n",
    "        network = tf.nn.rnn_cell.DropoutWrapper(network, output_keep_prob=0.5)\n",
    "        network = tf.nn.rnn_cell.MultiRNNCell([network] * num_layers)\n",
    "        \n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_ph)\n",
    "        inputs =  unpack_sequence(inputs)\n",
    "       \n",
    "        rnn_output, states_op = tf.nn.rnn(network,inputs,dtype=tf.int32)\n",
    "        #rnn_output, states_op = tf.nn.dynamic_rnn(network,inputs,dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        #rnn_output = pack_sequence(rnn_output)\n",
    "        #state_op = pack_sequence(states_op)\n",
    "        \n",
    "        output_op = tf.transpose(rnn_output, [1, 0, 2])\n",
    "        #output_op = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "        #output_op = tf.nn.softmax(tf.matmul(rnn_output[-1], weight) + bias)\n",
    "\n",
    " \n",
    "        tf.histogram_summary(\"weights\", weight)\n",
    "        tf.histogram_summary(\"biases\", bias)\n",
    "        tf.histogram_summary(\"output\",  output_op)\n",
    "        results = [weight, bias]\n",
    "        return output_op, states_op, results\n",
    "\n",
    "\n",
    "def loss(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss_op = - tf.reduce_sum(supervisor_ph * tf.log(output_op))\n",
    "        tf.scalar_summary(\"loss\", loss_op)\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "def training(loss_op):\n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        training_op = optimizer.minimize(loss_op)\n",
    "        return training_op\n",
    "\n",
    "def accuracy(output_op, supervisor_ph):\n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct_prediction = tf.equal(tf.argmax(output_op,1), tf.argmax(supervisor_ph,1))\n",
    "        accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        tf.scalar_summary(\"accuracy\", accuracy_op)\n",
    "        return accuracy_op\n",
    "\n",
    "def calc_accuracy(accuracy_opp, X, t):\n",
    "    inputs, targets = create_batch(len(X), X, t)\n",
    "    pred_dict = {\n",
    "        input_ph:  inputs,\n",
    "        supervisor_ph: targets\n",
    "    }\n",
    "    accurecy = sess.run(accuracy_op, feed_dict=pred_dict)\n",
    "    print(accurecy)\n",
    "\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_ph = tf.placeholder(tf.int32, [None, sequences_length, num_of_input_nodes], name=\"input\")\n",
    "    supervisor_ph = tf.placeholder(tf.float32, [None, class_num], name=\"supervisor\")\n",
    "\n",
    "    output_op, states_op, datas_op = inference(input_ph)\n",
    "    loss_op = loss(output_op, supervisor_ph)\n",
    "    training_op = training(loss_op)\n",
    "    accuracy_op = accuracy(output_op, supervisor_ph)\n",
    "\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "        summary_writer = tf.train.SummaryWriter(\"tmp/tensorflow_log\", graph=sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(num_of_training_epochs):\n",
    "            #inputs, supervisors = create_batch(batch_size, training_predictors_tf , training_classes_tf)\n",
    "            inputs, supervisors = training_predictors_tf , training_classes_tf\n",
    "            train_dict = {\n",
    "                input_ph:   inputs,\n",
    "                supervisor_ph: supervisors\n",
    "            }\n",
    "            sess.run(training_op, feed_dict=train_dict)\n",
    "\n",
    "            if (epoch) % 1000 == 0:\n",
    "                #summary_str, loss = sess.run([summary_op, loss_op], feed_dict=train_dict)\n",
    "                loss = sess.run(loss_op, feed_dict=train_dict)\n",
    "                print(\"train#{}, loss: {}\".format(epoch, loss))\n",
    "                #summary_writer.add_summary(summary_str, epoch)\n",
    "                if (epoch) % 5000 == 0:\n",
    "                    calc_accuracy(output_op, test_predictors_tf, test_classes_tf)\n",
    "        calc_accuracy(output_op, X_test, t_test)\n",
    "        datas = sess.run(datas_op)\n",
    "        saver.save(sess, \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
